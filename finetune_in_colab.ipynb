{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hflf-3ozZCrE"
      },
      "source": [
        "# LLM 微调入门 (Qwen2.5-1.5B + Ruozhiba)\n",
        "\n",
        "本 Notebook 将指导你在 Google Colab 上运行 `finetune-simple-llm` 项目。\n",
        "\n",
        "**注意**：请确保在菜单栏中选择 **修改 (Runtime)** -> **更改运行时类型 (Change runtime type)**，并将硬件加速器设置为 **T4 GPU** (或更好)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRtclJOkZCrF"
      },
      "source": [
        "## 1. 安装依赖\n",
        "\n",
        "安装项目所需的 Python 库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FN971SHUZCrF"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers>=4.37.0 datasets peft accelerate bitsandbytes scipy tiktoken einops modelscope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRWfASPyZCrG"
      },
      "source": [
        "## 2. 克隆代码仓库\n",
        "\n",
        "从 GitHub 克隆项目代码。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iiJ8OwGIZCrG",
        "outputId": "14610e05-3e02-43bf-b07b-a707912627e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning https://github.com/metaxiuyi/finetune-simple-llm.git...\n",
            "Clone successful!\n",
            "Current working directory: /content/finetune-simple-llm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "repo_url = \"https://github.com/metaxiuyi/finetune-simple-llm.git\"\n",
        "repo_name = \"finetune-simple-llm\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning {repo_url}...\")\n",
        "    ret = os.system(f\"git clone {repo_url}\")\n",
        "    if ret != 0:\n",
        "        print(\"\\n\\033[91mError: Git clone failed!\\033[0m\")\n",
        "        print(\"Most likely reason: The repository is PRIVATE.\")\n",
        "        print(\"Please go to GitHub Settings -> General -> Danger Zone -> Change visibility and set it to PUBLIC.\")\n",
        "    else:\n",
        "        print(\"Clone successful!\")\n",
        "else:\n",
        "    print(f\"Repository {repo_name} already exists. Skipping clone.\")\n",
        "\n",
        "if os.path.exists(repo_name):\n",
        "    os.chdir(repo_name)\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"\\n\\033[91mFailed to enter repository directory.\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMSVhsFIZCrG"
      },
      "source": [
        "## 3. 数据准备\n",
        "\n",
        "下载并处理弱智吧数据集。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MgsKHlE1ZCrH",
        "outputId": "2449f0f9-6fa8-4d13-dfa9-7d0cdae130cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset 'LooksJuicy/ruozhiba'...\n",
            "README.md: 100% 500/500 [00:00<00:00, 2.08MB/s]\n",
            "ruozhiba_qa.json: 591kB [00:00, 66.8MB/s]\n",
            "Generating train split: 100% 1496/1496 [00:00<00:00, 34416.12 examples/s]\n",
            "Dataset loaded. Size: 1496\n",
            "Formatting and saving to ruozhiba_formatted.jsonl...\n",
            "Data preparation complete.\n"
          ]
        }
      ],
      "source": [
        "!python prepare_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0ScTwCZCrH"
      },
      "source": [
        "## 4. 模型微调 (LoRA)\n",
        "\n",
        "开始训练模型。在 T4 GPU 上，这可能需要几分钟到十几分钟。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s9H02O7IZCrH",
        "outputId": "7fa319a0-ace4-40e3-a02c-b76dbe26e250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from ruozhiba_formatted.jsonl...\n",
            "Generating train split: 1496 examples [00:00, 203287.72 examples/s]\n",
            "Loading model Qwen/Qwen2.5-1.5B-Instruct...\n",
            "config.json: 100% 660/660 [00:00<00:00, 3.56MB/s]\n",
            "tokenizer_config.json: 7.30kB [00:00, 24.6MB/s]\n",
            "vocab.json: 2.78MB [00:00, 87.4MB/s]\n",
            "merges.txt: 1.67MB [00:00, 120MB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "tokenizer.json: 7.03MB [00:00, 142MB/s]\n",
            "model.safetensors: 100% 3.09G/3.09G [00:49<00:00, 62.7MB/s]\n",
            "Loading weights: 100% 338/338 [00:06<00:00, 53.72it/s, Materializing param=model.norm.weight] \n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.66MB/s]\n",
            "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
            "Preprocessing dataset...\n",
            "Map: 100% 1496/1496 [00:00<00:00, 2523.39 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetune-simple-llm/train.py\", line 124, in <module>\n",
            "    main()\n",
            "  File \"/content/finetune-simple-llm/train.py\", line 104, in main\n",
            "    trainer = Trainer(\n",
            "              ^^^^^^^^\n",
            "TypeError: Trainer.__init__() got an unexpected keyword argument 'tokenizer'\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UnXrKtzZCrH"
      },
      "source": [
        "## 5. 推理测试\n",
        "\n",
        "加载微调后的模型进行对话。\n",
        "注意：`inference.py` 是交互式的，但在 Colab 的非交互式单元格中可能运行不便。我们可以直接在 Notebook 中运行推理代码。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul5T2Z2vZCrH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "def run_inference():\n",
        "    base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    adapter_path = \"qwen_ruozhiba_finetuned\"\n",
        "\n",
        "    print(f\"Loading base model: {base_model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
        "    try:\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load adapter: {e}\")\n",
        "        model = base_model\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # 测试问题\n",
        "    test_questions = [\n",
        "        \"钢筋混凝土是荤菜还是素菜？\",\n",
        "        \"蓝牙耳机坏了，去医院挂牙科还是耳科？\",\n",
        "        \"如果我吃了感冒药，是不是就不能生病了？\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Start Inference Testing\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\nUser: {question}\")\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        print(f\"Assistant: {response}\")\n",
        "\n",
        "run_inference()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}